---
title: "01_generate_data"
author: "Leeya Pressburger"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    fig_caption: yes
    number_sections: yes
    toc: true
    toc_depth: 2
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

### R setup

Load necessary packages, read in an ini file and initiate core, and set seed for reproducibility.

```{r intro, warning=FALSE, message=FALSE}
library(hector)
library(relaimpo)
library(dplyr)
library(tidyr)
library(ggplot2)
library(gridExtra)
library(ggExtra)
library(GGally)
library(bookdown)
library(data.table) 

theme_set(theme_light())

source("trackingC_common.R")

# Set random number generator to allow for reproducibility 
set.seed(10)
```

### Create runlist of parameter draws

Currently, we are looking at seven Hector parameters: `BETA(), Q10_RH(), ECS(),
NPP_FLUX0(), AERO_SCALE(), DIFFUSIVITY(), and a LUC emissions scalar`. 
We created a runlist with `r SSP_runs[MAIN_SCENARIO]` random
draws in a normal (or lognormal, where applicable) distribution.

ECS (equilibrium climate sensitivity) and ocean heat diffusivity are 
notably correlated with each other, so we use a joint PDF for these.  

ECS is log-normally distributed, but we will transform it to fit a 
normal distribution by taking the log of the entire distribution.
If we do this, we can use MASS::mvrnorm() which saves us a LOT of work in
writing a custom rejection sampling routine...  

We will then transform the final values back to fit a lognormal distribution
by taking the exponential of our ECS values

```{r runlist}
# Create runlist of parameters of interest

# Function to access lognorm parameters with a desired mean and standard deviation
# Reference: https://msalganik.wordpress.com/2017/01/21/making-sense-of-the-rlnorm-function-in-r/
lognorm <- function(m, sd){
  mn <- log(m^2 / sqrt(sd^2 + m^2))
  stdev <- sqrt(log(1 + (sd^2 / m^2)))
  v <- c(mn, stdev)
}

# Function to create a tibble of random draws given a specified number to produce
generate_params <- function(run_numbers){
  runs <- length(run_numbers)
  tibble(
    run_number = run_numbers,
    # Keenan et al. (2021) (0.54, 0.03)
    # Match prior of trendy models
    "BETA" = rnorm(runs, mean = 0.28, sd = 0.1),
    
    # Davidson and Janssens (2006)
    "Q10_RH" = rlnorm(runs, lognorm(1.5, 1.0)[1], lognorm(1.5, 1.0)[2]),
    
    # Ito (2011)
    "NPP_FLUX0" = rnorm(runs, mean = 56.2, sd = 14.3),
    
    # Smith et al. (2020)
    "AERO_SCALE" = rnorm(runs, mean = 1.01, sd = 0.1),
    
    # Land use change emissions scaling parameter, to account for uncertainties in values for LUC
    # Friedlingstein et al 2021 state: 
    # Cumulative CO2 emissions from land-use changes (ELUC) for 1850-2020 were 200 ± 65 GtC...
    # ...large spread among individual estimates of 140 GtC (updated H&N2017), 270 GtC (BLUE),
    # and 195 GtC (OSCAR) for the three bookkeeping models and a similar wide estimate of 190 ± 60 GtC for the DGVMs
    # Cumulative LUC_EMISSIONS() in Hector from 1850 to 2020 total 168 GtC
    # We want to create a random distribution to scale our Hector emissions
    # to account for the large uncertainties
    # Our bounds for a reasonable scalar therefore are (270/168) and (140/168)
    # Or, 0.83 < x < 1.61 
    # The following distribution roughly aligns with the above criteria
    # Using a lognormal distribution because we do not want negative or near-zero values
    "LUC_SCALE" = rlnorm(runs, lognorm(1.3, 0.2)[1], lognorm(1.3, 0.2)[2])
  )
}

# Create a single runlist for all SSPs
# First create scenario identifier column (across all scenarios), 
# and then generate the run numbers 
scenarios <- rep(names(SSP_runs), SSP_runs) %>% as.vector()
runlist <- tibble(run_number = 1:sum(SSP_runs), ssp = scenarios)

# Next, create and join a parameter dataframe
param_sce <- generate_params(runlist$run_number)
runlist <- left_join(runlist, param_sce, by = "run_number")
```

``` {r joint-params, fig.cap = " The final joint distribution of ECS and diffusivity. The individual parameter distributions are shown on the x and y axes."}
# Joint PDF for ECS and diffusivity
# Sherwood et al (2020)
ecs_vals_lognorm <- rlnorm(nrow(runlist), 
                           lognorm(3.25, 0.65)[1], 
                           lognorm(3.25, 0.65)[2])
# Transform ECS distribution to log(distribution)
ecs_vals_to_norm <- log(ecs_vals_lognorm)
diffusion_vals <- rnorm(nrow(runlist), mean = 2.3, sd = 0.23)

DESIRED_PCOR <- -0.75  # ECS and diffusivity should be negatively correlated
# https://en.wikipedia.org/wiki/Covariance_and_correlation
# To calculate covariance from the desired correlation, multiply the 
# correlation by the square of the variance of each variable - the standard deviation
# https://web.stanford.edu/class/archive/cs/cs109/cs109.1178/lectureHandouts/150-covariance.pdf
desired_cov <- DESIRED_PCOR * sd(ecs_vals_to_norm) * sd(diffusion_vals)
# Set up the variance-covariance matrix
Sigma <- matrix(c(var(ecs_vals_to_norm), 
                  desired_cov, desired_cov, 
                  var(diffusion_vals)),
                2, 2)
# ...and generate values--twice as many as needed because we toss out
# negative ECS values below (see note above)
joint_vals <- MASS::mvrnorm(n = nrow(runlist) * 2,
                            c(mean(ecs_vals_to_norm),
                              mean(diffusion_vals)), Sigma)
colnames(joint_vals) <- c("ECS", "DIFFUSIVITY")
jv <- as_tibble(joint_vals)

# Filter and bind to the runlist
jv %>% 
  mutate(ECS = exp(ECS)) %>%
  filter(ECS > 0) %>% 
  sample_n(nrow(runlist)) %>%
  bind_cols(runlist, .) ->
  runlist

# Visualize the final joint distribution of ECS and DIFFUSIVITY
p <- ggplot(runlist, aes(ECS, DIFFUSIVITY)) + 
  geom_point(alpha = 0.2) + 
  geom_density_2d() +
  ggtitle("Joint distribution of ECS and DIFFUSIVITY")
ggExtra::ggMarginal(p, type = "histogram", alpha = 0.2)

```

``` {r log-dist, fig.cap = " Comparing the original, random, lognormal distribution for ECS with the final transformed version, the exponential of the log of ECS."}

# Visualize proper lognormal versus transformed distributions for ECS
tibble(lognorm = ecs_vals_lognorm, transformed = runlist$ECS) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(value, fill = name)) + geom_density(alpha = 0.5) +
  ggtitle("Comparison of (proper) lognormal with transformed distribution for ECS")
```

## Run the model

Run the model and store outputs and tracking data.

```{r functions, message=FALSE, cache=TRUE}
run_hector <- function(pdata, c, sce) {
  
  # Function to set multiple parameter values for Hector, run a core, and retrieve tracking data
  # pdata will be the runlist specified above
  # c is a Hector newcore environment
  
  # For each parameter within each run, set its value and units
  message("\tSetting", appendLF = FALSE)
  for(p in colnames(pdata)) {
    message(" ", p, appendLF = FALSE)
    # If the parameter is a scaling constant, multiply and set scaled values
    if(p %in% scalar_vector) {
      setvar(c, luc$year, LUC_EMISSIONS(), (luc$value * pdata[p][[1]]), "Pg C/yr")
    } else {
      # Otherwise, simply set the value of the parameter itself
      setvar(c, NA, do.call(p, list()), pdata[p][[1]], units_vector[p])
    }
  }
  message("  ")
  # Set a tracking date, reset and run core
  setvar(c, NA, TRACKING_DATE(), TRACKING_START, NA)
  reset(c)
  run(c)
  # Access and save tracking data, model outputs
  out <- list()
  out$tdata <- get_tracking_data(c) %>% 
    filter(year %in% OUTPUT_YEARS, 
           # Save only emissions and their destinations or the atmosphere 
           # and its sources to keep file size down
           source_name == "earth_c" | pool_name == "atmos_co2",
           # remove unneeded rows to shrink outputs
           year %% SAVE_EVERY_YEARS == 0)
  
  out$results <- fetchvars(c, OUTPUT_YEARS, 
                           c("CO2_concentration", "RF_tot",
                             "RF_CO2", "global_tas", "luc_emissions")) %>% 
    # remove unneeded rows to shrink outputs
    filter(year %% SAVE_EVERY_YEARS == 0)
  
  # Remove unneeded columns to save space
  out$results$scenario <- NULL
  out$results$units <- NULL
  
  return(out)
}

# Run function for each row of the runlist

# Create destination list
out <- list()

# List for the rows that error
errors <- list()

# For each row of the runlist, apply the run_hector() function
# If there is an error, record which row triggers the error, and skip to the next

# Record start time 
start_time <- Sys.time()

for(s in names(SSP_files)) {
  # Filter runlist by scenario
  s_runlist <- runlist %>% filter(ssp == s)
  # Initiate new core, by scenario
  core <- newcore(SSP_files[[s]])
  # Run core, extract LUC emissions data for later multiplication by LUC_SCALE
  run(core)
  luc <- fetchvars(core, core$strtdate:core$enddate, LUC_EMISSIONS())
  
  for(row in seq_len(nrow(s_runlist))) {
    message(s, " ", row, "/", nrow(s_runlist))
    # Create unique ssp + run_number identifier for each row so that data 
    # does not get overwritten
    id <- paste0(s, "_", row)
    
    outp <- try(run_hector(s_runlist[row,][-c(1, 2)], core, s))
    if(class(outp) == "try-error") { 
      # if there is an error, record row numbers that error
      message("\tError!")
      errors[[id]] <- s_runlist$run_number[[row]]
    } else {
      # Save output by id
      out[[id]] <- outp
      # Add in run_number column - can no longer bind_rows() by run_number
      # because data is grouped by id
      out[[id]]$results$run_number <- s_runlist$run_number[[row]]
      out[[id]]$tdata$run_number <- s_runlist$run_number[[row]]
    }
  }
  shutdown(core)
}

# Compute difference in time between start and after loop is run
tm <- difftime(Sys.time(), start_time, units = "secs") %>% round(1)

# Collapse errors into a single vector
errors <- unlist(errors)

```

Doing `r nrow(runlist)` runs took `r tm` seconds 
or `r round(tm / (sum(SSP_runs)), 1)` s/job.

`r length(errors)` runs had errors.


## Process data

Get lists from above into nice dataframes.

```{r data}
if(!length(out)) {
  stop("No output data; probably no model runs were done because cache=TRUE?")
}

# Get output data frames
# First, access tracking data
tdata <- lapply(out, function(x) x$tdata)
trk_output <- bind_rows(tdata) %>% as_tibble

# Then, access model output data
results <- lapply(out, function(x) x$results)
model_output <- bind_rows(results) %>% as_tibble()

```

```{r destination-data}
# To save memory in our main file, we manipulate the destination fraction code here
# Step 1: compute source_quantity = pool_value * source_fraction
# Step 2: for each year and source_name, compute destination_fraction

dest_output <- trk_output %>%
  filter(source_name == "earth_c", pool_name != "earth_c") %>%
  mutate(source_quantity = pool_value * source_fraction) %>%
  group_by(year, run_number) %>%
  summarize(destination_fraction = source_quantity / sum(source_quantity),
            pool_name = pool_name,
            .groups = "drop") %>% 
  ungroup()

```

# Save outputs

```{r output-files}
fwrite(runlist, "./output_files/runlist_test.csv", row.names = FALSE)
fwrite(model_output, "./output_files/model_output_test.csv", row.names = FALSE)
fwrite(trk_output, "./output_files/trk_output_test.csv", row.names = FALSE)
fwrite(dest_output, "./output_files/dest_output_test.csv", row.names = FALSE)
```

`runlist` size = `r format(object.size(runlist), "Mb")`

`model_output` size = `r format(object.size(model_output), "Mb")`

`trk_output` size = `r format(object.size(trk_output), "Mb")`


# The End

```{r info}
sessionInfo()
```
